# LogFlow 综合日志分析工作流配置
# 演示完整的日志处理、错误检测、数据分析和通知流程

workflow:
  id: comprehensive_log_analysis
  name: "综合日志分析工作流"
  description: "完整的日志处理流程：数据采集、预处理、错误检测、性能分析、关联处理和多渠道通知"
  version: "2.1.0"
  author: "LogFlow Team"
  metadata:
    category: "日志分析"
    tags: ["日志", "错误检测", "性能监控", "实时分析"]
    complexity: "high"

# 全局配置
globalConfig:
  timeout: 120000              # 2分钟超时
  retryCount: 3               # 失败重试3次
  logLevel: "INFO"            # 日志级别
  enableMetrics: true         # 启用指标收集
  parallelExecution: true     # 启用并行执行
  maxConcurrentNodes: 6       # 最大并发节点数

# 节点定义
nodes:
  # ================================
  # 1. 数据输入和预处理阶段
  # ================================
  
  # 输入节点 - 接收外部数据
  - id: workflow_input
    name: "工作流输入"
    type: input

    position:
      x: 100
      y: 100
    config:
      inputKey: "log_files"
      outputKey: "raw_input"
      dataType: "object"
      defaultValue:
        source: "application_logs"
        batch_size: 1000

  # 插件节点 - 文件数据源
  - id: file_data_source
    name: "日志文件读取器"
    type: plugin

    position:
      x: 300
      y: 100
    config:
      pluginType: "file"
      outputKey: "file_logs"
      # 文件插件特定配置
      filePath: "/var/log/application.log"
      format: "json"
      encoding: "UTF-8"
      maxLines: 10000
      skipLines: 0

  # 插件节点 - 模拟数据源（用于测试）
  - id: mock_data_source
    name: "模拟数据生成器"
    type: plugin

    position:
      x: 300
      y: 200
    config:
      pluginType: "mock"
      outputKey: "mock_logs"
      # 模拟插件特定配置
      mockType: "mixed_logs"
      recordCount: 5000
      errorRate: 15
      timeRange: 24

  # 脚本节点 - 数据预处理
  - id: data_preprocessor
    name: "数据预处理器"
    type: script

    position:
      x: 500
      y: 150
    config:
      scriptEngine: "javascript"
      inputKey: "file_logs"
      outputKey: "cleaned_logs"
      script: |
        // 数据清洗和标准化
        var logs = input || [];
        var cleaned = [];
        
        for (var i = 0; i < logs.length; i++) {
          var log = logs[i];
          
          // 跳过空记录
          if (!log || !log.message) {
            continue;
          }
          
          // 标准化时间戳
          if (log.timestamp) {
            log.normalizedTime = new Date(log.timestamp).toISOString();
          }
          
          // 标准化日志级别
          if (log.level) {
            log.level = log.level.toUpperCase();
          }
          
          // 添加处理标记
          log.processed = true;
          log.processedAt = utils.now();
          
          cleaned.push(log);
        }
        
        // 设置统计信息到上下文
        context.setData('preprocessing_stats', {
          original_count: logs.length,
          cleaned_count: cleaned.length,
          processed_at: utils.now()
        });
        
        logger.info('数据预处理完成: ' + cleaned.length + ' 条记录');
        
        // 返回清洗后的数据
        cleaned;
      parameters:
        max_records: 50000
        enable_deduplication: true

  # ================================
  # 2. 分析和诊断阶段
  # ================================

  # 诊断节点 - 错误检测
  - id: error_detector
    name: "错误检测分析器"
    type: diagnosis

    position:
      x: 700
      y: 100
    config:
      diagnosisType: "error_detection"
      inputKey: "cleaned_logs"
      outputKey: "error_analysis"
      errorPatterns:
        - "ERROR"
        - "FATAL"
        - "Exception"
        - "Failed"
        - "Timeout"
        - "Connection refused"
      realTimeMode: false

  # 诊断节点 - 性能分析
  - id: performance_analyzer
    name: "性能分析器"
    type: diagnosis

    position:
      x: 700
      y: 200
    config:
      diagnosisType: "performance_analysis"
      inputKey: "cleaned_logs"
      outputKey: "performance_analysis"
      slowThreshold: 2000.0  # 2秒响应时间阈值
      realTimeMode: false

  # 脚本节点 - 自定义业务逻辑分析
  - id: business_logic_analyzer
    name: "业务逻辑分析器"
    type: script

    position:
      x: 700
      y: 300
    config:
      scriptEngine: "javascript"
      inputKey: "cleaned_logs"
      outputKey: "business_analysis"
      script: |
        // 业务逻辑分析
        var logs = input || [];
        var analysis = {
          user_activities: {},
          api_calls: {},
          critical_events: [],
          summary: {}
        };
        
        // 分析用户活动
        var userActivities = {};
        var apiCalls = {};
        var criticalEvents = [];
        
        for (var i = 0; i < logs.length; i++) {
          var log = logs[i];
          
          // 用户活动统计
          if (log.user_id) {
            if (!userActivities[log.user_id]) {
              userActivities[log.user_id] = { count: 0, actions: [] };
            }
            userActivities[log.user_id].count++;
            if (log.action) {
              userActivities[log.user_id].actions.push(log.action);
            }
          }
          
          // API调用统计
          if (log.api_endpoint) {
            if (!apiCalls[log.api_endpoint]) {
              apiCalls[log.api_endpoint] = { count: 0, avg_response_time: 0, errors: 0 };
            }
            apiCalls[log.api_endpoint].count++;
            if (log.response_time) {
              apiCalls[log.api_endpoint].avg_response_time += log.response_time;
            }
            if (log.level === 'ERROR') {
              apiCalls[log.api_endpoint].errors++;
            }
          }
          
          // 关键事件检测
          if (log.level === 'FATAL' || 
              (log.message && log.message.includes('CRITICAL')) ||
              (log.response_time && log.response_time > 10000)) {
            criticalEvents.push({
              timestamp: log.timestamp,
              message: log.message,
              severity: log.level,
              response_time: log.response_time
            });
          }
        }
        
        // 计算平均响应时间
        for (var endpoint in apiCalls) {
          if (apiCalls[endpoint].count > 0) {
            apiCalls[endpoint].avg_response_time = 
              apiCalls[endpoint].avg_response_time / apiCalls[endpoint].count;
          }
        }
        
        analysis.user_activities = userActivities;
        analysis.api_calls = apiCalls;
        analysis.critical_events = criticalEvents;
        analysis.summary = {
          total_logs: logs.length,
          unique_users: Object.keys(userActivities).length,
          unique_apis: Object.keys(apiCalls).length,
          critical_events_count: criticalEvents.length,
          analysis_time: utils.now()
        };
        
        logger.info('业务逻辑分析完成，发现 ' + criticalEvents.length + ' 个关键事件');
        
        analysis;
      parameters:
        include_user_analysis: true
        include_api_analysis: true
        critical_response_threshold: 10000

  # ================================
  # 3. 关联处理阶段
  # ================================

  # 关联节点 - 调用子工作流进行深度分析
  - id: deep_analysis_workflow
    name: "深度分析工作流"
    type: reference

    position:
      x: 900
      y: 150
    config:
      workflowId: "deep_log_analysis"
      executionMode: "async"
      timeoutMs: 180000  # 3分钟超时
      inputMapping:
        cleaned_logs: "input_data"
        error_analysis: "error_context"
        performance_analysis: "perf_context"
      outputMapping:
        detailed_report: "deep_analysis_result"
        recommendations: "improvement_suggestions"

  # 脚本节点 - 结果聚合
  - id: result_aggregator
    name: "结果聚合器"
    type: script

    position:
      x: 1100
      y: 200
    config:
      scriptEngine: "javascript"
      inputKey: "multiple_inputs"  # 特殊输入键，表示多个输入
      outputKey: "final_report"
      script: |
        // 聚合所有分析结果
        var errorAnalysis = context.getData('error_analysis') || {};
        var perfAnalysis = context.getData('performance_analysis') || {};
        var businessAnalysis = context.getData('business_analysis') || {};
        var deepAnalysis = context.getData('deep_analysis_result') || {};
        var preprocessStats = context.getData('preprocessing_stats') || {};
        
        var finalReport = {
          execution_info: {
            workflow_id: context.getWorkflowId(),
            execution_id: context.getExecutionId(),
            generated_at: utils.now(),
            processing_stats: preprocessStats
          },
          error_summary: {
            total_errors: errorAnalysis.errorCount || 0,
            error_rate: errorAnalysis.errorRate || '0%',
            critical_errors: (errorAnalysis.errors || []).filter(e => e.level === 'FATAL').length
          },
          performance_summary: {
            slow_requests: perfAnalysis.slowRequestCount || 0,
            avg_response_time: perfAnalysis.averageResponseTime || 0,
            performance_issues: perfAnalysis.performanceIssues || []
          },
          business_summary: businessAnalysis.summary || {},
          deep_analysis: deepAnalysis,
          recommendations: context.getData('improvement_suggestions') || [],
          alert_level: 'INFO'  // 默认警告级别
        };
        
        // 确定警告级别
        var criticalErrors = finalReport.error_summary.critical_errors;
        var slowRequests = finalReport.performance_summary.slow_requests;
        
        if (criticalErrors > 10 || slowRequests > 100) {
          finalReport.alert_level = 'CRITICAL';
        } else if (criticalErrors > 5 || slowRequests > 50) {
          finalReport.alert_level = 'WARNING';
        } else if (criticalErrors > 0 || slowRequests > 10) {
          finalReport.alert_level = 'INFO';
        }
        
        logger.info('最终报告生成完成，警告级别: ' + finalReport.alert_level);
        
        finalReport;

  # ================================
  # 4. 通知和输出阶段
  # ================================

  # 通知节点 - 控制台输出
  - id: console_notification
    name: "控制台通知"
    type: notification

    position:
      x: 1300
      y: 100
    config:
      providerType: "console"
      inputKey: "final_report"
      messageTemplate: |
        === LogFlow 分析报告 ===
        执行时间: {{execution_info.generated_at}}
        警告级别: {{alert_level}}
        
        错误统计:
        - 总错误数: {{error_summary.total_errors}}
        - 错误率: {{error_summary.error_rate}}
        - 严重错误: {{error_summary.critical_errors}}
        
        性能统计:
        - 慢请求数: {{performance_summary.slow_requests}}
        - 平均响应时间: {{performance_summary.avg_response_time}}ms
        
        业务统计:
        - 处理日志数: {{business_summary.total_logs}}
        - 唯一用户数: {{business_summary.unique_users}}
        - API端点数: {{business_summary.unique_apis}}
      priority: "normal"

  # 通知节点 - 文件输出
  - id: file_output
    name: "报告文件输出"
    type: notification

    position:
      x: 1300
      y: 200
    config:
      providerType: "file"
      inputKey: "final_report"
      messageTemplate: "{{toJson(this)}}"  # 输出完整JSON
      priority: "normal"
      # 文件提供者特定配置
      provider.filePath: "/tmp/logflow_reports/analysis_report_{{now}}.json"
      provider.format: "json"
      provider.append: false

  # 通知节点 - 上下文输出（供其他系统使用）
  - id: context_output
    name: "上下文数据输出"
    type: notification

    position:
      x: 1300
      y: 300
    config:
      providerType: "context"
      inputKey: "final_report"
      priority: "normal"
      # 上下文提供者特定配置
      provider.contextKey: "workflow_final_result"

# 节点连接关系
connections:
  # 数据流入和预处理
  - from: workflow_input
    to: file_data_source

    
  - from: file_data_source
    to: data_preprocessor

    
  # 并行分析阶段
  - from: data_preprocessor
    to: error_detector

    
  - from: data_preprocessor
    to: performance_analyzer

    
  - from: data_preprocessor
    to: business_logic_analyzer

    
  # 深度分析
  - from: error_detector
    to: deep_analysis_workflow

    condition: "result.errorCount > 0"  # 只有发现错误时才执行深度分析
    
  - from: performance_analyzer
    to: deep_analysis_workflow

    condition: "result.slowRequestCount > 10"  # 只有发现性能问题时才执行
    
  # 结果聚合
  - from: error_detector
    to: result_aggregator

    
  - from: performance_analyzer
    to: result_aggregator

    
  - from: business_logic_analyzer
    to: result_aggregator

    
  - from: deep_analysis_workflow
    to: result_aggregator

    
  # 通知输出
  - from: result_aggregator
    to: console_notification

    
  - from: result_aggregator
    to: file_output

    
  - from: result_aggregator
    to: context_output

